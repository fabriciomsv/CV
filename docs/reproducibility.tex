% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\usepackage{graphicx}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage[natbibapa]{apacite}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{subfig}
\usepackage[spanish]{babel}
\addto\captionsspanish{\renewcommand{\figurename}{Figura}}
\addto\captionsspanish{\renewcommand{\lstlistingname}{Bloque de código}}
\usepackage{fancyhdr}
\pagestyle{fancy}
\chead[encabezado en el centro en páginas pares]{USO DE AMBIENTES REPRODUCIBLES PARA LA VALIDACIÓN DE MODELOS DE APRENDIZAJE ESTADÍSTICO}
\providecommand{\keywords}[1]{
  \small
  	\textbf{\textit{Keywords---}} #1
}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figura}
\else
  \newcommand\figurename{Figura}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Tabla}
\else
  \newcommand\tablename{Tabla}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Código}
\newcommand*\listoflistings{\listof{codelisting}{Lista de Códigos}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Uso de ambientes reproducibles para la validación de modelos de aprendizaje estadístico},
  pdfauthor={Fabricio Machado, Mauro Loprete},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Uso de ambientes reproducibles para la validación de modelos de
aprendizaje estadístico}
\author{Fabricio Machado, Mauro Loprete}
\date{}

\begin{document}
\maketitle


\begin{center}
        \large Uso de ambientes reproducibles para la validación de modelos de aprendizaje estadístico.
        \\
        Una implementación en Docker.
\end{center}

\begin{center}
        Fabricio Machado, Mauro Loprete
\end{center}

\keywords{Reproducibilidad, Portabilidad, Análisis discriminante, Validación cruzada, MLOps}

\begin{abstract}
  Este trabajo presenta la implementación de un modelo de aprendizaje estadístico apoyado en la herramienta Docker. Se combinan técnicas de clustering y análisis discriminante para clasificar individuos en grupos sociales y se recurre al uso de ambientes reproducibles en Docker para validar dicha clasificación y garantizar su replicabilidad. La relevancia del trabajo no se limita únicamente a los resultados del modelo y sus aportes a la ciencia social, sino que incluye también la experiencia acumulada en el uso de herramientas modernas y sus potencialidades para el diseño de flujos de análisis más robustos y trazables.
\end{abstract}

\newpage

\begin{abstract}
    Abstract en inglés
\end{abstract}

\newpage

\section{Introducción}\label{introducciuxf3n}

El uso de técnicas de aprendizaje estadístico se ha extendido a
múltiples campos de investigación. Estos métodos requieren una adecuada
fundamentación de las decisiones adoptadas para su implementación y,
además, pueden ser computacionalmente muy exigentes. Por este motivo, se
han desarrollado herramientas complementarias a los softwares habituales
que permiten afrontar estos desafíos y, al mismo tiempo, fomentan la
reproducibilidad. En este contexto, la reproducibilidad no debe
entenderse como un fin en sí mismo, sino como parte del proceso de
trabajo a incorporar desde el inicio, lo que facilita la verificación de
resultados, mejora la colaboración y aumenta la credibilidad del trabajo
ante la comunidad académica.

Utilizamos entornos reproducibles en Docker para implementar y validar
un modelo de aprendizaje estadístico en el marco de una investigación en
ciencias económicas. En este documento, sistematizamos dicha
implementación, destacando los métodos utilizados y los resultados
obtenidos; además, introducimos los conceptos básicos para trabajar con
Docker y presentamos una guía práctica para su uso.

Docker es una herramienta de software que permite ejecutar aplicaciones
de manera aislada, consistente y reproducible, reduciendo los problemas
habituales asociados a versiones, dependencias y diferencias entre
entornos. Facilita que un programa y todos los componentes necesarios
para su funcionamiento se configuren una sola vez y puedan ejecutarse de
forma prácticamente idéntica en distintos contextos: desde un equipo
personal hasta servidores locales o infraestructuras en la nube. Gracias
a ello, simplifica las tareas de prueba y despliegue, mejora la
colaboración entre desarrolladores y favorece la automatización de
entornos complejos.

Usar Docker en investigación puede ser muy útil porque nos permite
congelar un entorno de trabajo completo (sistema, librerías, versiones
de R, Stata, Python, LaTeX, etc.) y volver a él siempre que lo
necesitemos, sin depender de la computadora específica ni de cómo esté
configurada. Eso hace que los proyectos sean reproducibles en el tiempo
(volver a correr los archivos de replicación de un paper años después de
realizados) y más fáciles de compartir con otras personas (coautores,
tribunales, replicadores) sin dedicar horas a instalar dependencias o
resolver conflictos de versiones.

En un proyecto típico podemos combinamos varios lenguajes y herramientas
(R para procesar datos y gráficos, Stata para partes del análisis,
Python para algún script auxiliar, LaTeX/Quarto para el documento,
etc.). Además, solemos tener distintas máquinas involucradas: portátil
personal, PC del trabajo, servidor de la facultad o la nube. Docker
encaja bien con esa forma de trabajar porque permite definir un entorno
único para el proyecto y ejecutarlo igual en todos esos lugares. En vez
de escribir instrucciones largas del tipo ``instalar tal versión de R,
tal paquete, tal versión de Stata'', definimos el entorno una vez, lo
empaquetamos y cualquiera puede levantarlo y correr el proyecto
completo.

Docker es compatible con prácticamente todos los softwares y
herramientas que usamos para investigar: se puede trabajar con R y
RStudio Server, Python y Jupyter, Stata (en servidores donde tengas
licencia), y sistemas de compilación de documentos como LaTeX o Quarto.
En la práctica, lo que hace Docker es ejecutar estos programas dentro de
un entorno Linux aislado, pero nosotros seguimos trabajando desde
nuestro navegador o editor habitual (como RStudio, Positron o VS Code),
conectándonos al contenedor. Esto permite crear, por ejemplo, un
contenedor que tenga R + todos los paquetes necesarios + TeX para
compilar el paper, otro con Stata y sus do-files, y así garantizar que
los análisis y el documento se puedan reconstruir exactamente igual en
cualquier máquina que tenga Docker instalado.

¿Qué aporta este documento? - Ejemplo de aplicación práctica en un
modelo estadístico. - Guía para el uso de Docker en investigación. -
Sistematización de los beneficios de usar esta herramienta. -
Acercamiento de la herramienta a la comunidad IESTA.

La reproducibilidad es clave para verificar y validar nuestros
resultados una vez finalizada una investigación. Pero también una forma
de trabajo que podemos adoptar desde el inicio: documentar los pasos,
fijar versiones de software, automatizar scripts y organizar el código y
los datos de manera sistemática. Optar por este camino tiene beneficios
importantes: facilita la revisión interna, mejora la colaboración entre
coautores, reduce errores difíciles de rastrear y aumenta la
credibilidad del trabajo ante la comunidad académica, que puede evaluar,
replicar o extender nuestros resultados con mayor confianza.

La combinación entre los métodos y datos elegidos implicó un alto
requerimiento computacional que motivó el acercamiento inicial a los
entornos reporducibles.

\section{Modelo de aprendizaje
estadístico}\label{modelo-de-aprendizaje-estaduxedstico}

Partimos de un análisis sobre la desigualdad económica cuyo objetivo es
clasificar a los individuos en grupos sociales a partir de variables
relevantes para estudiar su participación en el ingreso laboral
(Machado, 2023). En una primera etapa aplicamos
\textbf{análisis de clúster}, que nos permite detectar grupos
relativamente homogéneos entre los perceptores de ingresos laborales en
un año base, utilizando variables que resumen las características de los
trabajadores y de sus puestos de trabajo. En una segunda etapa empleamos
\textbf{análisis discriminante} para construir, a partir de esos
clústeres, una regla de clasificación que pueda aplicarse a otros años y
así seguir la evolución de los grupos y su participación en el ingreso
laboral. Dado que esta regla se estima en un conjunto de datos y luego
se aplica sobre información externa, utilizamos técnicas de
\textbf{validación cruzada} para evaluar su desempeño y reducir el
riesgo de sobreajuste.

A continuación se detallan los métodos que forman parte del proceso de
implementación en Docker que se seguirá en las secciones posteriores, el
cual incluye los desarrollos del análisis discriminante y de la
validación cruzada. El paso previo de análisis de clúster puede
consultarse en el Anexo 1.

\subsection{Datos}\label{datos}

La fuente de datos utilizada es la Encuesta Continua de Hogares (ECH)
elaborada por el Instituto Nacional de Estadística (INE). Se emplea
información de personas y hogares para los años 2006, 2011, 2019 y 2021,
seleccionados por representar momentos distintos de la dinámica
económica reciente (salida de la crisis, auge económico, período previo
a la pandemia y pospandemia). En cada año se consideran las personas
ocupadas de 20 años o más de todo el país con ingresos laborales,
definiéndose ocupación como haber trabajado al menos una hora en la
semana de referencia. Se excluye a quienes trabajaron en programas
públicos de empleo, a los miembros no remunerados del hogar y a los
individuos con datos faltantes en alguna de las variables empleadas en
el análisis de clúster. En este último caso, la ausencia de datos impide
computar dicho análisis; en los demás casos, los registros se apartan
debido a su escasa magnitud relativa y a que no se cuenta con evidencia
de que constituyan grupos clave para diferenciar a los trabajadores. En
particular, los no remunerados no participan de la masa salarial, que es
el objeto central del estudio.

Las muestras finales incluyen 92.735 individuos en 2006, 53.210 en 2011,
43.565 en 2019 y 9.007 en 2021. Una vez expandidas mediante el
ponderador anual, las estimaciones poblacionales ascienden a 1.128.540
individuos en 2006, 1.411.419 en 2011, 1.441.540 en 2019 y 1.396.668 en
2021. La notoria reducción en el número de observaciones para 2021 se
atribuye a los cambios en la metodología de la ECH introducidos en 2020
(INE, 2020). Para el análisis se seleccionan variables que recogen
información sobre las tareas realizadas en el trabajo, el sector de
actividad, la categoría de ocupación, la informalidad, las horas
trabajadas, el nivel educativo, la propiedad de la vivienda y el cobro
de rentas.

\subsection{Análisis discriminante}\label{anuxe1lisis-discriminante}

Esta aplicación pretende llegar a una misma clasificación de grupos para
todos los años seleccionados, manteniendo las características
distintivas de cada uno. A partir de un conjunto de datos con la
pertenencia a los grupos conocida (año base donde se aplicó el análisis
de cluster), se deriva una regla para asignar las observaciones a los
grupos haciendo mínima la probabilidad de clasificar incorrectamente. Se
clasifican los datos de 2006, 2019 y 2021 para formar la misma cantidad
de grupos y con la misma caracterización resultante del análisis de
cluster realizado en 2011, que fue seleccionado como año base por
presentar la clasificación más estable (Ver Anexo 1).

Entre las distintas funciones discriminantes se eligió el discriminante
logístico dado que todas las variables son cualitativas (Peña, 2002).
Dado que se espera encontrar más de dos grupos, suponemos que la
variable que indica las subpoblaciones proviene de una distribución
multinomial. Se presentan a continuación las probabilidades logarítmicas
entre grupos (James et al., 2021):

\begin{center}
    $log( \frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = \beta_{k0} + \beta_{k1} x_{1} + ... + \beta_{kp} x_{p}$ 
\end{center}

En nuestra especificación se predefinieron cuatro grupos (\(K = 4\)),
por lo que dejando fuera al de referencia nos quedan tres funciones de
enlace para \(k = 1, 2, 3\) (\(k = 1, ..., K-1\)). Se denotan como
\(X = (X_{1}, ..., X_{p})\) el conjunto de variables utilizadas, siendo
\(p\) la cantidad de categorías de todas las variables, dejando siempre
una categoría libre por la trampa de las dummys (\(p = 23\)).

Se estiman 69 parámetros \((K-1)(p)\) que representan la probabilidad de
pertenecer al los grupos \(k = 1, 2, 3\) en relación con el grupo de
referencia según cada una de las características
\(X_{1}, ..., X_{p}\).\footnote{Los parámetros del modelo se estiman
  bajo el método de máxima verosimilitud con técnicas numéricas de
  optimización, en concreto el método Brodyen-Fletcher-Goldfarb-Shanno.}

Para revisar la significación conjunta del modelo se plantea,
\(H_{0}) \hat\beta_{0} = \hat\beta_{1} = ... = \hat\beta_{23} = 0\),
contra \(H_{a})\) algún \(\hat{\beta} \neq 0\), para esto se utiliza el
estadístico de razón de verosimilitud que para esta aplicación sigue una
distribución \(\chi^{2}_{69}\) (\(\chi^{2}_{(K-1)(p)}\)).

Con la información obtenida del año base se pretende discriminar a los
individuos de los demás años. Los individuos se asignan a un grupo o a
otro comparando las probabilidades a priori, con esta regla de
clasificación el grupo más probable es aquel donde su verosimilitud es
más alta. Para evaluar el modelo se utiliza una medida de precisión
definida como la cantidad de datos bien clasificados sobre el total.
Valores altos de la misma implican un buen desempeño del modelo

\subsection{Validación cruzada}\label{validaciuxf3n-cruzada}

Como es de interés clasificar datos externos, para evitar problemas de
sobreidentificación se utiliza el método de
\textbf{validación cruzada $k-folds$} (James et. al., 2021). Se dividen
los datos aleatoriamente en \(k\) pliegues de un tamaño similar, se
estima la precisión tomando uno de los pliegues para el testeo y los
restantes \(k-1\) para el entrenamiento, el procedimiento se repite
\(k\) veces haciendo variar el pliegue de testeo de tal forma que todos
los datos hayan sido utilizados como testeo y entrenamiento. Como
resultado se tienen \(k\) estimaciones de la tasa de error y se tomara
el promedio del mismo como una buena medida del ajuste sobre el total de
los datos.

El trabajo fue principalmente realizado en R, desde el procesamiento de
datos con la librería tidytable y la implementación de los métodos con
la librería \textbf{nnet} (v7.3-19; Venables y Ripley, 2002).
Adicionalmente se crearon las funciones en R necesarias para la
aplicación de la validación cruzada. Se explotó la independencia de los
modelos de validación cruzada k-folds para realizarlos mediante
computación en paralelo con la librería parallel.

\section{Implementación}\label{implementaciuxf3n}

Dado el volumen de datos y la intensidad computacional requerida, se
optó por el uso de Docker para garantizar la reproducibilidad y
portabilidad del entorno de desarrollo. Docker nos permitió encapsular
todas las dependencias del proyecto dentro de un contenedor, asegurando
que el proceso pueda ejecutarse de manera consistente en diferentes
entornos, ya sea en servidores en la nube, en un servidor de cómputo
local o en computadoras personales con diferentes sistemas operativos
(Matthias y Kane, 2015).

El ambiente de trabajo se configuró mediante un \textbf{Dockerfile} que
define todas las herramientas y bibliotecas necesarias para la ejecución
del modelo. Este archivo incluye desde la instalación de R y sus
paquetes relevantes, hasta la configuración de las dependencias del
sistema operativo. De esta forma, cualquier usuario puede reproducir el
ambiente exacto simplemente construyendo y ejecutando el contenedor.

Para optimizar el rendimiento y facilitar la integración del modelo en
entornos de producción, se aplicaron principios de Machine Learning
Operations (MLOps). \textbf{MLOps} es un enfoque que combina prácticas
de desarrollo de software (DevOps) con metodologías específicas de
Machine Learning, con el objetivo de automatizar, escalar y gestionar
los ciclos de vida de los modelos de aprendizaje automático de manera
eficiente.

Dentro de este marco, el procesamiento en paralelo jugó un papel
crucial. Se utilizó el paquete \textbf{parallel} (R Core Team, 2023) de
R para distribuir el procesamiento de datos y la validación cruzada del
modelo en múltiples núcleos de CPU, lo que permitió acelerar
significativamente los tiempos de ejecución y manejar grandes volúmenes
de datos de manera más efectiva. La integración de estas prácticas
asegura que el modelo no solo sea reproducible, sino también escalable y
adaptable a diferentes entornos.

Docker funciona como una capa de abstracción a nivel del sistema
operativo, que permite empaquetar aplicaciones y sus dependencias en
contenedores ligeros y portables. De este modo, el software se ejecuta
de forma consistente en distintos entornos (equipo local, servidores o
nube). Docker se ejecuta mediante comandos en la terminal (CLI) o a
través de interfaces gráficas (Docker Desktop) y consta de varios
componentes clave que facilitan la creación, distribución y ejecución de
contenedores.

Los principales componentes de Docker son:

\begin{itemize}
\tightlist
\item
  \texttt{Dockerfile}: archivo de texto con instrucciones para construir
  la imagen.
\item
  Imagen: plantilla estática y versionada del entorno completo.
\item
  Contenedor: instancia en ejecución de la imagen.
\item
  Registro: repositorio para almacenar y compartir imágenes (Docker Hub,
  GHCR).
\end{itemize}

A continuación se presenta en la \ref{fig:dockerlifecycle} un diagrama
simplificado del ciclo de vida de Docker, que ilustra los conceptos
clave y el flujo típico de trabajo con contenedores:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/docker-lifecycle.png}
  \caption{Ciclo de vida simplificado de Docker.}
  \label{fig:dockerlifecycle}
\end{figure}

Para crear un contenedor se define un \texttt{Dockerfile} con la imagen
base y los pasos para instalar dependencias, copiar código y configurar
el entorno. El resultado es una imagen; un contenedor es la instancia en
ejecución de esa imagen.

En términos prácticos, resulta útil retener algunas ideas esenciales.
Una imagen es la plantilla inmutable del entorno (sistema, librerías y,
cuando corresponde, parte del código) identificada por \texttt{repo:tag}
y por un \texttt{digest} (\texttt{sha256:}) que garantiza identidad
exacta entre máquinas. Un contenedor es el proceso que ejecuta esa
imagen de forma aislada; toda escritura dentro del contenedor es
temporal, salvo que se utilice un volumen montado desde el equipo
anfitrión, que es donde deben guardarse datos y resultados. El
Dockerfile es la receta para construir la imagen con instrucciones como
\texttt{FROM}, \texttt{RUN} o \texttt{COPY}, y el registro es el
repositorio donde se versionan y comparten imágenes con coautores y
replicadores. También es recomendable limitar recursos (CPU y memoria)
para obtener ejecuciones controladas y comparables.

Desde el punto de vista operativo, Docker utiliza un servicio del
sistema que crea y administra contenedores, apoyándose en el kernel para
aislar procesos y recursos, y en un sistema de archivos por capas
(copy‑on‑write) que ahorra espacio y acelera las compilaciones. El ciclo
de trabajo es simple y repetible: se construye la imagen, se ejecuta
montando datos y carpeta de resultados, se observan los registros o se
ingresa al contenedor cuando es necesario, se persisten los artefactos
en el volumen del equipo anfitrión y, finalmente, se limpian recursos no
utilizados. La idea central es que cualquier cambio que ocurra dentro
del contenedor es efímero; por ello, los resultados deben escribirse
siempre en una ruta montada desde el host (por ejemplo,
\texttt{/output}).

La diferencia respecto a otros sistemas de virtualización como máquinas
virtuales es que Docker comparte el kernel del sistema anfitrión, lo que
reduce el uso de recursos y acelera el arranque. Esta característica lo
vuelve especialmente adecuado para desarrollar, desplegar y escalar
análisis reproducibles en investigación. Las máquinas virtuales, en
cambio, cargan un sistema operativo completo, consumen más CPU, memoria
y almacenamiento, y requieren herramientas de administración
específicas. Para la mayoría de los flujos de investigación orientados a
reproducibilidad y portabilidad, los contenedores ofrecen una solución
más ágil y suficiente.

Para sostener la reproducibilidad a lo largo del tiempo, se recomienda
etiquetar imágenes con fecha o versión (y, cuando sea crítico,
referenciarlas por \texttt{digest}), congelar paquetes (R:
\texttt{renv}; Python: \texttt{requirements.txt}) y guardar
\texttt{sessionInfo()} o \texttt{pip\ freeze} junto a los resultados.
Los datos y las credenciales no deben incorporarse en la imagen: se
montan a través de un volumen con \texttt{-v} y se suministran por
variables de entorno o archivos \texttt{.env}. También es conveniente
fijar semillas aleatorias, normalizar configuraciones regionales y zonas
horarias y ejecutar como usuario no privilegiado (o con
\texttt{-\/-user\ \$(id\ -u):\$(id\ -g)}). Documentar los comandos
exactos de \texttt{docker\ build} y \texttt{docker\ run} (y los límites
de recursos) en un README o Makefile mejora la trazabilidad.

El bloque de código~\ref{lst-commands-utils} contiene lo esencial para
inspeccionar, ejecutar, observar y limpiar:

\begin{codelisting}

\caption{\label{lst-commands-utils}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inspección}
\ExtensionTok{docker}\NormalTok{ images }\CommentTok{\# Lista todas las imagenes locales}
\ExtensionTok{docker}\NormalTok{ ps }\AttributeTok{{-}a} \CommentTok{\# Lista todos los contenedores (también los detenidos)}

\CommentTok{\# Ejecutar con límites y variables}
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}{-}rm} \AttributeTok{{-}it} \AttributeTok{{-}v} \StringTok{"}\VariableTok{$PWD}\StringTok{"}\NormalTok{:/work }\AttributeTok{{-}w}\NormalTok{ /work }\AttributeTok{{-}{-}cpus}\OperatorTok{=}\NormalTok{4 }\AttributeTok{{-}m}\NormalTok{ 8g }\DataTypeTok{\textbackslash{}}
  \AttributeTok{{-}{-}env{-}file}\NormalTok{ .env ghcr.io/fabriciomch/cv:latest /bin/bash}

\CommentTok{\# Logs y acceso}
\ExtensionTok{docker}\NormalTok{ logs }\AttributeTok{{-}f} \OperatorTok{\textless{}}\NormalTok{nombre\_o\_id}\OperatorTok{\textgreater{}} \CommentTok{\# Seguir logs en tiempo real}
\ExtensionTok{docker}\NormalTok{ exec }\AttributeTok{{-}it} \OperatorTok{\textless{}}\NormalTok{nombre\_o\_id}\OperatorTok{\textgreater{}}\NormalTok{ /bin/bash }\CommentTok{\# Acceder a la shell del contenedor}

\CommentTok{\# Copiar artefactos (scripts, resultados, etc.)}
\ExtensionTok{docker}\NormalTok{ cp }\OperatorTok{\textless{}}\NormalTok{nombre\_o\_id}\OperatorTok{\textgreater{}}\NormalTok{:/work/output/resultados.csv ./output/}

\CommentTok{\# Limpieza}
\ExtensionTok{docker}\NormalTok{ system prune }\AttributeTok{{-}f} \CommentTok{\# Limpia volumenes, redes y contenedores detenidos}
\ExtensionTok{docker}\NormalTok{ image prune }\AttributeTok{{-}f} \CommentTok{\# Limpia imagenes no usadas}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

A modo de cierre, es útil recordar algunas pautas para evitar errores
frecuentes: utilizar siempre una imagen etiquetada y el Dockerfile
versionado; fijar dependencias y semillas; montar datos y escribir
resultados en \texttt{/work/output}; documentar el comando de ejecución
y los recursos asignados; y conservar registros y metadatos de sesión
junto a los outputs. Si los resultados no aparecen, suele deberse a que
no se montó \texttt{-v\ "\$PWD":/work} o no se escribió en la ruta
montada; si faltan paquetes, deben instalarse en la imagen base o en un
paso de inicialización; si surgen problemas de permisos, ejecutar con
\texttt{-\/-user\ \$(id\ -u):\$(id\ -g)}; y si hay limitaciones de
memoria, aumentar \texttt{-m} o reducir el tamaño de los lotes.

\subsection{Ejemplo}\label{ejemplo}

Como ejemplo concreto, a continuación se muestra un Dockerfile (ver
bloque de código~\ref{lst-dockerfile-example}) para obtener el resultado
del análisis detallado en la sección \textbf{PONER REF}. Usa como base
\texttt{eddelbuettel/r2u:22.04} (Ubuntu + R con binarios precompilados),
instala los paquetes mencionados en \textbf{poner ref} (\texttt{nnet},
\texttt{vroom}, \texttt{here}, \texttt{tidytable}, \texttt{magrittr}),
crea una estructura de carpetas para el proyecto e incluye los datos
(\texttt{/home/ubuntu/model/\{data,R,output\}}) a utilizar, instala
paquetes de R, copia archivos y define un comando por defecto que
ejecuta el script principal.

\begin{codelisting}

\caption{\label{lst-dockerfile-example}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{FROM}\NormalTok{ eddelbuettel/r2u:22.04  }\CommentTok{\# Ubuntu 22.04}

\CommentTok{\# Estructura de proyecto}
\KeywordTok{RUN} \FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ /home/ubuntu/model/}\DataTypeTok{\{data}\OperatorTok{,}\DataTypeTok{R}\OperatorTok{,}\DataTypeTok{output\}}
\KeywordTok{WORKDIR}\NormalTok{ /home/ubuntu/model}

\CommentTok{\# Paquetes de R del sistema (binarios)}
\KeywordTok{RUN} \ExtensionTok{apt{-}get}\NormalTok{ update }\KeywordTok{\&\&} \ExtensionTok{apt{-}get}\NormalTok{ install }\AttributeTok{{-}y} \DataTypeTok{\textbackslash{}}
\NormalTok{    r{-}cran{-}nnet }\DataTypeTok{\textbackslash{}}
\NormalTok{    r{-}cran{-}vroom }\DataTypeTok{\textbackslash{}}
\NormalTok{    r{-}cran{-}here }\DataTypeTok{\textbackslash{}}
\NormalTok{    r{-}cran{-}tidytable }\DataTypeTok{\textbackslash{}}
\NormalTok{    r{-}cran{-}magrittr }\DataTypeTok{\textbackslash{}}
  \KeywordTok{\&\&} \FunctionTok{rm} \AttributeTok{{-}rf}\NormalTok{ /var/lib/apt/lists/}\PreprocessorTok{*}

\CommentTok{\# Copiar archivos (opcional; preferible montar con {-}v en ejecución)}
\KeywordTok{COPY}\NormalTok{ R/* R/}
\KeywordTok{COPY}\NormalTok{ data/ data/ }
\KeywordTok{COPY}\NormalTok{ main.R main.R}

\CommentTok{\# Variables de entorno de ejemplo}
\KeywordTok{ARG}\NormalTok{ RUN\_MODE=prod}
\KeywordTok{ENV}\NormalTok{ RUN\_MODE=$\{RUN\_MODE\}}

\CommentTok{\# Comando por defecto (En este script se ejecutan los modelos con CV)}
\KeywordTok{CMD}\NormalTok{ [}\StringTok{"Rscript"}\NormalTok{, }\StringTok{"main.R"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Una vez creada la imagen a partir de este \texttt{Dockerfile}, los
contenedores ejecutarán el script \texttt{main.R}, que implementa el
modelo de aprendizaje estadístico con validación cruzada y guarda los
resultados en la carpeta \texttt{output/}. El comando por defecto puede
ser sobrescrito al ejecutar el contenedor (mediante
\texttt{docker\ run}) si se desea correr otro script o ingresar a una
shell interactiva (\texttt{docker\ run\ imagen\ -it\ bash}) dentro del
contenedor.

Al construir esta imagen, los comandos de ejecución deben montar el
proyecto en la misma ruta definida como \texttt{WORKDIR} para que los
scripts y outputs queden en el lugar esperado. Como puede verse en el
bloque de código~\ref{lst-dockerfile-usage}.

\begin{codelisting}

\caption{\label{lst-dockerfile-usage}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Construir la imagen local}
\ExtensionTok{docker}\NormalTok{ build }\AttributeTok{{-}t}\NormalTok{ cv\_multinom .}

\CommentTok{\# Abrir una shell en la ruta del proyecto dentro del contenedor}
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}{-}rm} \AttributeTok{{-}it} \DataTypeTok{\textbackslash{}}
  \AttributeTok{{-}v} \StringTok{"}\VariableTok{$PWD}\StringTok{"}\NormalTok{:/home/ubuntu/model }\AttributeTok{{-}w}\NormalTok{ /home/ubuntu/model }\DataTypeTok{\textbackslash{}}
\NormalTok{  cv\_multinom /bin/bash}

\CommentTok{\# Ejecutar el script principal y escribir resultados en ./output}
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}{-}rm} \DataTypeTok{\textbackslash{}}
  \AttributeTok{{-}v} \StringTok{"}\VariableTok{$PWD}\StringTok{"}\NormalTok{:/home/ubuntu/model }\AttributeTok{{-}w}\NormalTok{ /home/ubuntu/model }\DataTypeTok{\textbackslash{}}
\NormalTok{  cv\_multinom Rscript main.R}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Este Dockerfile tiene un ejemplo básico para que devuelva el accuracy
del modelo, definido en la sección \textbf{poner ref} como la forma
correcta de validar los resultados. En otros casos puede ser util
guardar otras métricas, modelos intermedios o datos procesados en la
carpeta \texttt{output/}, que está montada desde el equipo anfitrión.

La imagen ya se encuentra construida y publicada en base a la ultima
versión del código en el repositorio de
\href{https://github.com/FabricioMch/CV}{GitHub} y mediante GitHub
Actions se actualiza automáticamente al crear nuevas etiquetas.

\section{Ejecución}\label{ejecuciuxf3n}

Para trabajar con la imagen pública de este proyecto alojada en GHCR
(\texttt{ghcr.io/fabriciomch/cv}) no es necesario modificar nada: basta
con descargarla, ejecutarla montando la carpeta del proyecto y, si se
desea mantener una variante local, construir una imagen propia a partir
de un Dockerfile. De manera esquemática, un flujo típico de trabajo
puede resumirse del siguiente modo.

Pasos de uso (imagen GHCR):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Obtener la imagen

  \begin{itemize}
  \item
    Para imágenes públicas (como en este caso), ejecutar:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ pull ghcr.io/fabriciomch/cv:latest}
\end{Highlighting}
\end{Shaded}
  \item
    Si fuera privada, autenticarse previamente en GitHub Container
    Registry (GHCR) con un personal access token (PAT) asociado a la
    cuenta de GitHub.
  \end{itemize}
\item
  Vincular el proyecto local

  \begin{itemize}
  \item
    Desde la carpeta del proyecto (donde residen código y datos),
    ejecutar:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}{-}rm} \AttributeTok{{-}it} \AttributeTok{{-}v} \StringTok{"}\VariableTok{$PWD}\StringTok{"}\NormalTok{:/work }\AttributeTok{{-}w}\NormalTok{ /work ghcr.io/fabriciomch/cv:latest /bin/bash}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  Ejecutar el script de análisis

  \begin{itemize}
  \item
    Por ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}{-}rm} \AttributeTok{{-}v} \StringTok{"}\VariableTok{$PWD}\StringTok{"}\NormalTok{:/work }\AttributeTok{{-}w}\NormalTok{ /work ghcr.io/fabriciomch/cv:latest Rscript main.R}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  (Opcional) Congelar una variante propia

  \begin{itemize}
  \item
    Clonar el repositorio, adaptar el Dockerfile si es necesario y
    construir:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ build }\AttributeTok{{-}t}\NormalTok{ cv\_multinom }\AttributeTok{{-}{-}build{-}arg}\NormalTok{ test\_arg=TRUE .}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\end{enumerate}

Para la ejecución cotidiana y colaborativa, resulta útil acompañar el
flujo de trabajo con herramientas auxiliares. La integración continua
(GitHub/GitLab CI) permite construir y publicar imágenes automáticamente
al crear etiquetas; un Makefile facilita encapsular comandos repetitivos
como \texttt{docker\ run} o \texttt{docker\ build}; y servicios como
Amazon EC2 o AWS Batch resultan apropiados para ejecutar los mismos
contenedores cuando se requiere mayor capacidad de cómputo.

Para obtener resultados de forma consistente:

\begin{itemize}
\tightlist
\item
  Descargar o actualizar la imagen y verificar la etiqueta/versión
  utilizada.
\item
  Ejecutar el contenedor montando el proyecto y escribir los outputs en
  la carpeta compartida.
\item
  Registrar logs, versiones de paquetes y metadatos de sesión junto a
  los resultados.
\end{itemize}

Para ilustrar la simplicidad de uso, el bloque de
código~\ref{lst-dockerfile-minimal} muestra los comandos mínimos para
descargar y ejecutar la imagen pública del proyecto, montando la carpeta
\texttt{output} para guardar los resultados generados por el script
principal.

\begin{codelisting}

\caption{\label{lst-dockerfile-minimal}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ pull ghcr.io/fabriciomch/cv:main}
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}v} \VariableTok{$\{PWD\}}\NormalTok{/output/:/home/ubuntu/model/output ghcr.io/fabriciomch/cv:main}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Este comando descarga la imagen pública y ejecuta el contenedor,
montando la carpeta \texttt{output} del proyecto local para guardar los
resultados generados por el script principal. Al tener en cuenta las
buenas prácticas mencionadas, se asegura que los análisis sean
reproducibles y portables entre distintos entornos y colaboradores.

\subsection{Desarrollo en la nube con GitHub
Codespaces}\label{desarrollo-en-la-nube-con-github-codespaces}

Una vez que se cuenta con la imagen Docker, existen múltiples formas de
utilizarla para ejecutar el análisis de forma reproducible tanto en
entornos locales (utilizando Docker Desktop o Docker Engine) como en la
nube como GitHub Codespaces.

GitHub Codespaces es un servicio que permite abrir un entorno de
desarrollo completo en la nube, basado en Visual Studio Code,
directamente desde un repositorio de GitHub. Este entorno puede
configurarse para utilizar contenedores Docker como base, lo que
facilita la ejecución de análisis reproducibles sin necesidad de
instalar software adicional en el equipo local.

A continuación se describen los pasos básicos para utilizar la imagen
Docker del proyecto en un entorno de GitHub Codespaces (Figura
\ref{fig:opencodespace}). Una vez abierto el Codespace, se puede
trabajar directamente con el código. El repositorio incluye un editor de
texto listo para ejecutar el contenedor Docker (Figura
\ref{fig:githubcodespace}). Esto puede ser especialmente útil para
colaborar con otros investigadores, docentes, alumnos, ya que todos
pueden acceder al mismo entorno sin preocuparse por las diferencias en
la configuración local de sus equipos.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/openCodeSpace.png}
  \caption{Apertura de un GitHub Codespace desde la interfaz del repositorio.}
  \label{fig:opencodespace}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/GithubCodeSpace.png}
  \caption{Entorno de desarrollo en la nube provisto por GitHub Codespaces.}
  \label{fig:githubcodespace}
\end{figure}

GitHub Codespaces puede entenderse, en este contexto, como un
complemento a los flujos basados en contenedores: permite abrir un
entorno de desarrollo efímero en la nube, definido a partir del propio
repositorio (por ejemplo mediante un archivo de configuración
\texttt{.devcontainer.json}), en el que se puede inspeccionar el código,
ajustar scripts y lanzar contenedores Docker con el mismo entorno que se
utiliza localmente.

Esto resulta especialmente útil para trabajo colaborativo, docencia o
talleres, ya que cualquier integrante del equipo puede acceder a un
entorno listo para usar sin instalar herramientas en su equipo personal.
No obstante, es importante tener en cuenta que GitHub Codespaces
presenta ciertas limitaciones en cuanto a recursos (CPU, memoria y
almacenamiento) y tiempo de ejecución, por lo que puede no resultar
adecuado para análisis muy intensivos o de larga duración. Servicios de
cómputo en la nube como Amazon EC2 u otras plataformas de
infraestructura como servicio suelen ser más apropiados cuando se
necesita alquilar capacidad de procesamiento para ejecutar cargas
pesadas en segundo plano (por ejemplo, corridas extensivas de validación
cruzada, simulaciones o entrenamientos de modelos de mayor porte). En
estos casos, la práctica recomendable es mantener la lógica del análisis
y la definición del entorno en una imagen Docker versionada, y desplegar
esa misma imagen tanto en el entorno local como en instancias de cómputo
en la nube, de modo que la decisión sobre el `dónde' ejecutar (equipo
propio o nube) no afecte al `cómo' se ejecuta el análisis.

\section{Comentarios finales}\label{comentarios-finales}

La implementación del modelo discriminante alcanzó una
\textbf{precisión del 94.35\%}, demostrando un excelente desempeño en la
clasificación de los individuos en los grupos sociales definidos.
Además, este trabajo nos permitió aprender y aplicar herramientas
modernas como Docker, Gitpod y Amazon EC2, lo que facilitó la
reproducibilidad y escalabilidad del proyecto.

Hemos comprobado que Docker puede ser una herramienta valiosa en la
investigación en Estadística y Ciencias Sociales, ya que permite
encapsular ambientes de desarrollo completos, asegurando que los
experimentos puedan replicarse de manera consistente en diferentes
entornos. Sus principales fortalezas son la reproducibilidad, el
aislamiento de dependencias, la portabilidad y la eficiencia
computacional. Particularmente es beneficioso incorporar esta
herramienta cuando se trabaja con combinaciones de datos y métodos que
requieren gran exigencia computacional y se detectan espacios para
paralelizar los procesos.

Recomendamos el uso de Docker y su complementación con otras
herramientas modernas.

La experiencia documentada muestra que la adopción de contenedores
Docker aporta beneficios tangibles para proyectos de métodos
cuantitativos: reduce fricciones en la instalación de dependencias,
estabiliza versiones de software, facilita la colaboración entre equipos
y mejora la trazabilidad de los resultados. Al encapsular el entorno de
ejecución, los análisis pueden replicarse en el tiempo y compartirse con
terceros sin la carga de reconstruir manualmente las condiciones de
trabajo. Esto se vuelve especialmente valioso cuando se combinan
múltiples lenguajes y herramientas (por ejemplo, R para preprocesamiento
y modelado, Python para tareas auxiliares, y servicios adicionales para
visualización o documentación) o cuando se requiere ejecutar los mismos
scripts en infraestructuras heterogéneas (estaciones de trabajo,
servidores institucionales o plataformas en la nube).

Desde el punto de vista metodológico, la estrategia combinada de
agrupamiento y análisis discriminante permite caracterizar perfiles de
individuos y trasladar esa caracterización a diferentes períodos, con
una métrica de desempeño clara y validada mediante procedimientos
estándar (k‑folds). La estimación de un discriminante logístico sobre
variables cualitativas resulta apropiada bajo supuestos razonables y
ofrece una regla de clasificación interpretable.

En síntesis, el uso de ambientes reproducibles con Docker constituye una
práctica recomendable para fortalecer la solidez empírica y la
portabilidad de los análisis en contextos académicos, a la vez que
habilita escalabilidad y mantenimiento a lo largo del ciclo de vida de
los proyectos.

\newpage

\subsection{Anexo}\label{anexo}

\subsection{Análisis de cluster}\label{anuxe1lisis-de-cluster}

Mediante esta técnica se pretende clasificar individuos en grupos. Para
esto es necesario definir una métrica que cuantifique la similitud entre
los individuos de la población. La variedad de métricas tienen distinta
sensibilidad a los datos atípicos, así como difieren en el tipo de
variables que pueden analizar entre cuantitativas y categóricas,
nominales u ordinales.

Dado que se trabajará con variables categóricas la métrica elegida es el
coeficiente de similitud de Jaccard, definido como el tamaño de la
intersección dividido por el tamaño de la unión de los conjuntos de
muestras:

\begin{equation*}
  J(A,B) = \frac{|A \bigcap  B|}{|A \bigcup B|} = \frac{|A \bigcap  B|}{|A| + |B| - |A \bigcup B|}
\end{equation*}

En su aplicación particular a este problema se requiere utilizarlo para
considerar similares a quienes compartan categorías de las variables. De
esta forma luego del análisis individuos que compartan un conjunto de
categorías estarán asignados a un mismo grupo. Analizando posteriormente
las características de los individuos pertenecientes a cada grupo, se
podrá llegar a una caracterización a partir de la cual compararlos.

Existen métodos de clusterización jerárquicos y no jerárquicos. Su
diferencia fundamental es que en los métodos jerárquicos la unión se
realiza entre clusters y una vez que los individuos pertenecen a un
cluster lo hacen hasta el final ya que se basan en proximidades locales,
mientras que los métodos no jerárquicos o de partición dependen de
definir de antemano la cantidad de grupos, asignando así para cada grupo
un centroide y su asociación con los individuos será con base en medidas
de proximidad. Como los métodos jerárquicos no son recomendados para
grandes conjuntos de datos se optará por uno no jerárquico (Guénoche et
al., 1991).

Los métodos no jerárquicos se caracterizan por comenzar con una primera
solución de clasificación, y luego de forma iterativa aplicar diferentes
combinaciones para obtener particiones más homogéneas.

El método a utilizar es K-Medoides o partición en torno a los medoides
(PAM), donde cada cluster está representado por el individuo mediano
candidato. Un medoide es un objeto representativo del conjunto de datos,
tal que su diferencia promedio con los demás objetos es mínima. Con la
cantidad \(k\) de grupos definida previamente se asignan \(k\) medoides
dentro de los \(n\) individuos, los demás individuos son asignados al
grupo del medoide más cercano, la asignación del medoide va cambiando
minimizando la distancia entre los puntos de los grupos y su medoide
designado. Como el arranque es aleatorio se recomienda aplicar más de
una vez el método para llegar a conclusiones válidas.

Dado el tamaño de los datos a utilizar se optará por la implementación
CLARA (Kaufman y Rousseeuw, 1990) de K-Medoides, que utiliza el enfoque
de muestreo. Considera una muestra fija de los datos y aplica en ella el
algoritmo PAM encontrando el conjunto óptimo de medoides para la
muestra. Se repite el proceso de muestreo y agrupación un número
predefinido de veces para minimizar el sesgo del muestreo. La cantidad
de muestras se definirá probando un valor creciente de las mismas y
verificando a partir de cual los resultados se estabilizan.

Como se mencionó anteriormente, este tipo de métodos depende de
predefinir la cantidad de grupos. Una alternativa para esto es maximizar
la silueta promedio. La silueta es un estadístico que determina cuán
similar es un individuo respecto al grupo al que pertenece en
comparación a los demás grupos. Presenta un rango acotado en valor
absoluto menor a la unidad. Un valor alto indica que el individuo es muy
similar al grupo, y un valor bajo indica que el individuo es muy
diferente al grupo. En términos funcionales puede expresarse como:

\begin{equation*}
  s(i) = \frac{b(i) - a(i)}{max\{a(i), b(i)\}}
\end{equation*}

donde \(a(i)\) es la distancia promedio de la observación \(i\) a los
objetos de su grupo y \(b(i)\) es la distancia promedio de la
observación \(i\) a los objetos de los demás grupos (distancia intra y
entre grupos). Una referencia a considerar a nivel agregado es la
silueta promedio para una cantidad de grupos dada, la cual se calcula
como la media de las siluetas de todos los grupos. Para obtener una
recomendación sobre la cantidad de grupos óptima en este tipo de
algoritmos se puede comparar la silueta promedio de cada configuración.
Aquella que obtenga un valor más alto será una buena recomendación de la
cantidad de grupos.

Dado que 2021 tiene dos relevamientos de datos semestrales a diferencia
de los damás años que son anuales, se decidió no tomarlo como año base
para aplicar el análisis de cluster. Los datos de 2006 fueron
descartados para este análisis por la constatación de datos faltantes en
la variable referente al tamaño de la empresa, considerada importante
para diferenciar los grupos. Los análisis de cluster realizados en los
años 2011 y 2019 dan como resultado una mejor clasificación de grupos en
el año 2011, donde se llega a un mayor valor de silueta promedio y no
hay mayores problemas de similaridad dentro de grupos particulares. Se
eligen por lo tanto los datos del año 2011 para realizar el análisis de
cluster y servir de año base para los análisis posteriores.

\textbf{\small Referencias:}

\begin{itemize}
\small
\item James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: Springer.
\item Matthias, K., \& Kane, S. P. (2015). Docker: Up \& Running: Shipping Reliable Containers in Production. O'Reilly Media, Inc.
\item Peña, D. (2002). Análisis de datos multivariantes (Vol. 24). Madrid: McGraw-hill.
\item R Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.
\item Venables WN, Ripley BD (2002). Modern Applied Statistics with S, Fourth edition. Springer, New York. https://www.stats.ox.ac.uk/pub/MASS4/.
\item ARCA-DPSS (s.f.). Manual de Ciencia Abierta — Capítulo Docker. Disponible en: https://arca-dpss.github.io/manual-open-science/docker-chapter.html
\end{itemize}




\end{document}
