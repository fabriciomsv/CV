---
title: "USO DE AMBIENTES REPRODUCIBLES PARA LA VALIDACIÓN DE MODELOS DE APRENDIZAJE ESTADÍSTICO"
author: "Fabricio Machado, Mauro Loprete"
len: es
format:
  pdf:
    documentclass: article
    documentclassoptions: "11pt"
    latex-engine: xelatex
    number-sections: true
    keep-tex: true
    bibliography: biblio.bib 
    pagestyle: plain
crossref: 
  sec-prefix: 'Sección'
  fig-prefix: 'Figura'
  tbl-prefix: 'Tabla'
  lst-prefix: 'Bloque de código'
  lol-title: "Lista de Códigos"
  lst-title: "Código"
  fig-title: "Figura"
  tbl-title: "Tabla"
  appendix-title: "Apéndice"
  appendix-delim: ":"
header-includes: |
  \usepackage{graphicx}
  \usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
  \usepackage{amsmath}
  \usepackage[natbibapa]{apacite}
  \usepackage{graphicx}
  \usepackage{listings}
  \usepackage{float}
  \usepackage{subfig}
  \usepackage[spanish]{babel}
  \addto\captionsspanish{\renewcommand{\figurename}{Figura}}
  \addto\captionsspanish{\renewcommand{\lstlistingname}{Bloque de código}}
  \usepackage{fancyhdr}
  \chead[encabezado en el centro en páginas pares]{}
  \providecommand{\keywords}[1]{
    \small
    	\textbf{\textit{Keywords---}} #1
  }
  \renewcommand{\BBAB}{y}
---

\begin{center}
		\keywords{Reproducibilidad, Portabilidad, Análisis discriminante, Validación cruzada, MLOps}
\end{center}

\begin{abstract}
  Este trabajo presenta la implementación de un modelo de aprendizaje estadístico apoyado en la herramienta Docker. Se combinan técnicas de clustering y análisis discriminante para clasificar individuos en grupos sociales y se recurre al uso de ambientes reproducibles en Docker para validar dicha clasificación y garantizar su replicabilidad. La relevancia del trabajo no se limita únicamente a los resultados del modelo y sus aportes a la ciencia social, sino que incluye también la experiencia acumulada en el uso de herramientas modernas y sus potencialidades para el diseño de flujos de análisis más robustos y trazables.
\end{abstract}

\newpage

\begin{abstract}
	Abstract en inglés
\end{abstract}

\newpage

# Introducción

El uso de técnicas de aprendizaje estadístico se ha extendido a múltiples campos de investigación. Estos métodos requieren fundamentar las decisiones tomadas para su implementación y, además, pueden ser computacionalmente muy exigentes (@hastie2009, @ISLR). Por este motivo, se han desarrollado herramientas complementarias a los programas y entornos de software habituales que permiten afrontar estos desafíos y, al mismo tiempo, fomentan la reproducibilidad. En este contexto, la reproducibilidad no debe entenderse como un objetivo final, sino como parte del proceso de trabajo a incorporar desde el inicio, lo que facilita la verificación de resultados, mejora la colaboración y aumenta la credibilidad del trabajo ante la comunidad académica (@peng2011, @stodden2014, @stodden2016).

Aunque la discusión sobre reproducibilidad y ciencia abierta ha ganado espacio en la literatura (@baker2016, @christensen2019), son todavía relativamente escasos los ejemplos detallados que muestren cómo llevar estas recomendaciones a la práctica en investigaciones aplicadas en estadística y ciencias sociales. Este trabajo busca contribuir en ese sentido, ofreciendo un caso de estudio que sirva como puente entre los principios generales de la reproducibilidad y las decisiones concretas que toman quienes trabajan con datos en su día a día. Al hacerlo, apunta en particular a la comunidad académica local y regional, donde la adopción de entornos reproducibles aún es incipiente, con el objetivo de facilitar su incorporación progresiva y fomentar la calidad y la transparencia de las investigaciones empíricas.

En este trabajo documentamos el uso de entornos reproducibles en Docker para implementar y validar un modelo de aprendizaje estadístico en el marco de una investigación en ciencias económicas. Partimos del trabajo previo presentado en @Machado2023DIE. El caso de estudio se basa en un modelo de análisis discriminante con validación cruzada k-fold, aplicado a datos de encuestas de hogares para clasificar trabajadores en grupos socioeconómicos. Si bien retomamos brevemente el contexto de la investigación y los métodos de aprendizaje estadístico allí aplicados, en esta ocasión nos centramos en el proceso de trabajo que suele quedar oculto en los documentos finales. En dicho proyecto, el uso de entornos reproducibles fue un componente integral de la investigación y se tradujo en ganancias concretas de eficiencia, reproducibilidad y portabilidad del análisis.

Describimos paso a paso la implementación del modelo en entornos reproducibles con Docker e introducimos los conceptos básicos necesarios para su uso. Además, incorporamos un componente interactivo al ofrecer una guía para descargar y ejecutar el proceso, disponible públicamente en GitHub Container Registry, y complementamos el ejemplo con otras herramientas como GitHub Codespaces, de modo que cualquier persona pueda adaptar el flujo de trabajo a sus propios proyectos. Con ello buscamos acercar a la comunidad académica al uso de estas herramientas, reduciendo la barrera de entrada y ofreciendo una guía práctica que facilite que quienes leen este documento puedan incorporarlas efectivamente en su actividad de investigación. De esta manera, el documento combina un aporte metodológico, al proponer un flujo de trabajo reproducible, con un aporte pedagógico orientado a facilitar la adopción de estas prácticas en la comunidad académica.

Docker es una herramienta de software que permite ejecutar aplicaciones en entornos aislados, consistentes y reproducibles, reduciendo los problemas habituales asociados a versiones, dependencias y diferencias entre sistemas (@boettiger2015). Facilita que un programa y todos los componentes necesarios para su funcionamiento se configuren una sola vez y puedan ejecutarse de forma prácticamente idéntica en distintos contextos (equipos personales, servidores institucionales o infraestructuras en la nube), lo que simplifica las tareas de prueba y despliegue, mejora la colaboración y favorece la automatización de entornos complejos (@rad2017).

En el contexto de la investigación empírica, donde es habitual combinar varios lenguajes y herramientas y trabajar en diferentes máquinas, Docker permite definir un entorno único para cada proyecto y ejecutarlo de forma homogénea en cualquier equipo. De este modo se favorece la reproducibilidad en el tiempo, al hacer posible volver a correr los archivos de replicación de un paper años después, y se facilita el intercambio con coautores, tribunales o replicadores, evitando dedicar horas a instalar dependencias o resolver conflictos de versiones. Además, Docker es compatible con prácticamente todos los programas y herramientas que se utilizan habitualmente en investigación aplicada, incluyendo, entre otros, R y RStudio Server, Python y Jupyter, Stata (en servidores con licencia) y sistemas de composición de documentos como LaTeX o Quarto.

En suma, nuestros objetivos son: (i) documentar un flujo de trabajo reproducible para la implementación y validación de un modelo de aprendizaje estadístico en ciencias sociales con Docker, y (ii) ofrecer una guía práctica para que otros investigadores puedan adaptar este enfoque a sus propios proyectos con esta herramienta.

El documento se organiza de la siguiente manera. La sección 2 presenta el marco conceptual y define con claridad los principales conceptos utilizados a lo largo del texto. La sección 3 describe el contexto de la investigación, los datos empleados, los modelos de aprendizaje estadístico considerados y los paquetes de R utilizados para su implementación. En la sección 4 se introducen los componentes principales de Docker y su funcionamiento, a partir del ejemplo de implementación del modelo presentado en la sección anterior. La sección 5 ofrece una guía para la ejecución del desarrollo disponible públicamente en GitHub Container Registry e introduce el uso de GitHub Codespaces para adaptar el flujo de trabajo a proyectos propios. Por último, la sección 6 recoge los comentarios finales y destaca tanto los resultados del modelo como los beneficios observados del uso de Docker en términos de eficiencia y reproducibilidad.

# Marco conceptual

En esta sección presentamos el marco conceptual que organiza el resto del trabajo y concreta, en términos operativos, las ideas introducidas en la sección anterior. Definimos y discutimos una serie de nociones que consideramos centrales para pensar la relación entre métodos de aprendizaje estadístico y buenas prácticas de investigación empírica: ciencia abierta, archivos de replicación, entorno de cómputo, reproducibilidad computacional, entorno reproducible, contenedores de software, portabilidad y flujo de trabajo reproducible. El objetivo es fijar un lenguaje común y explicitar cómo estos conceptos se articulan entre sí, de modo que el uso de Docker y los desarrollos posteriores puedan entenderse no solo como una solución técnica, sino como una forma concreta de materializar esos principios en el trabajo cotidiano con datos.

**Ciencia abierta:** La ciencia abierta es un conjunto de prácticas orientadas a hacer más transparentes y accesibles los distintos componentes del proceso de investigación (@ARCA): datos, código, materiales, resultados y, cada vez más, entornos de cómputo. No se limita a la simple publicación de artículos, sino que promueve que los archivos de replicación, los protocolos y las decisiones de análisis se compartan de forma tal que otros puedan inspeccionarlos, reutilizarlos o extenderlos. En ese marco, la transparencia sobre cómo se procesan los datos y en qué entorno se ejecutan los análisis se vuelve tan importante como los resultados finales. Esta visión es coherente con el enfoque de la ciencia abierta como “término paraguas” discutido por @fecher2013 y con las propuestas específicas para la investigación en ciencias sociales de @christensen2019.

**Archivos de replicación:** Los archivos de replicación (o materiales de replicación) reúnen los insumos necesarios para volver a ejecutar un análisis empírico: datos^[Cuando no puedan difundirse los datos, se recurre a versiones anonimizadas o instrucciones claras para acceder a ellos.], código de procesamiento y estimación, y documentación mínima sobre cómo reproducir las tablas y figuras principales. Desde la perspectiva de la ciencia abierta, los archivos de replicación son el soporte concreto de la reproducibilidad computacional: permiten que otras personas verifiquen los resultados de una investigación sin depender del entorno de trabajo original del autor. 

**Entorno de cómputo:** El entorno de cómputo, o entorno de software, es el conjunto de componentes técnicos sobre los cuales se ejecuta un análisis: sistema operativo, versiones de lenguajes (R, Python, Stata), bibliotecas y paquetes instalados, herramientas de compilación de documentos (LaTeX, Quarto), así como configuraciones específicas (localización, rutas, variables de entorno). En investigaciones empíricas complejas, este entorno rara vez es neutro: diferencias de versión o de configuración pueden hacer que un script falle o que ciertos resultados cambien marginalmente, aun cuando los datos y el código “parezcan” ser los mismos.

**Reproducibilidad computacional:** La reproducibilidad computacional se refiere a la capacidad de obtener exactamente los mismos resultados numéricos (tablas, figuras, estadísticas, métricas de desempeño) a partir de los mismos datos y el mismo código, incluso cuando el análisis se ejecuta en un momento distinto, por una persona distinta o en una máquina distinta (@sandve2013, @stodden2014). En la práctica, esto exige que los datos estén disponibles o adecuadamente documentados, que el código de procesamiento y análisis sea accesible y que el entorno de cómputo esté fijado o descrito con suficiente detalle como para que pueda ser recreado (@bechhofer2013).

**Entorno reproducible:** Un entorno reproducible es un entorno de cómputo que ha sido fijado y documentado de modo tal que otra persona pueda reconstruirlo y ejecutar los mismos análisis con la seguridad de obtener resultados equivalentes. No se trata solo de listar versiones de software, sino de proporcionar un mecanismo concreto para reconstruir ese entorno: scripts de instalación, archivos de configuración, descriptores de paquetes o un contenedor que ya integra todos los componentes necesarios. De esta manera, el entorno deja de ser una condición implícita del análisis y pasa a ser un objeto explícito y compartible.

**Contenedores de software:** Los contenedores de software son entornos aislados y ligeros que empaquetan aplicaciones junto con sus dependencias y configuraciones, compartiendo el kernel del sistema operativo anfitrión pero manteniendo su propio sistema de archivos y su propia pila de software (@boettiger2015). Se centran en proporcionar un espacio consistente para ejecutar aplicaciones de forma eficiente y portable, evitando conflictos de versiones y simplificando el despliegue en diferentes máquinas. En el contexto de la investigación empírica, los contenedores permiten agrupar el código de análisis, las librerías necesarias y las herramientas auxiliares en una unidad autocontenida que puede trasladarse y ejecutarse de forma homogénea en distintos entornos físicos.

**Portabilidad:** La portabilidad es la capacidad de trasladar y ejecutar un sistema o una aplicación en distintos equipos y plataformas sin necesidad de realizar adaptaciones significativas ni correr el riesgo de que falle por diferencias de configuración. Para un proyecto de investigación, un entorno portable implica que el análisis pueda pasar sin fricciones desde la máquina del investigador al servidor institucional o a una máquina en la nube, produciendo los mismos resultados sin reconfiguraciones extensas. La portabilidad es complementaria a la reproducibilidad: mientras esta se enfoca en poder rehacer los resultados, aquella se preocupa por poder hacerlo en múltiples contextos y con el menor costo de adaptación posible. El uso de contenedores como solución a los problemas de dependencia y portabilidad en el marco de la investigación reproducible ha sido discutido, entre otros, por @boettiger2015.

**Flujo de trabajo reproducible:** Un flujo de trabajo reproducible es la organización sistemática de los pasos de una investigación empírica (ingreso y limpieza de datos, transformación de variables, estimación de modelos, validación, generación de tablas y figuras, compilación del documento final) de forma que puedan ejecutarse de manera automática y determinista a partir de scripts y de un entorno de cómputo fijado (@wilkinson2016). Más allá de la idea general de “ordenar el proyecto”, un flujo reproducible implica que el proceso completo pueda rehacerse con un único comando o con un conjunto reducido de instrucciones bien documentadas, evitando operaciones manuales irrepetibles o decisiones implícitas que no quedan registradas. En este trabajo, el flujo de trabajo reproducible se materializa en la combinación de scripts (por ejemplo, en R o Stata) y contenedores Docker, de manera que tanto la lógica del análisis como el entorno en el que se ejecuta queden formalizados y puedan ser reutilizados por otros.


En conjunto, estos conceptos delinean el marco en el que se inscribe nuestro trabajo. La ciencia abierta aporta el horizonte normativo de transparencia y acceso; la reproducibilidad computacional fija el criterio mínimo para que los resultados puedan ser verificados; y la portabilidad asegura que ello pueda lograrse en distintos entornos físicos y organizacionales. Para alcanzar esos objetivos, resulta necesario explicitar el entorno de cómputo, transformarlo en un entorno reproducible y apoyarse en herramientas como los contenedores de software, que permiten encapsularlo y distribuirlo. Sobre esta base se construye el flujo de trabajo reproducible que proponemos, en el que tanto los pasos del análisis como el sistema en que se ejecutan se conciben como partes integrales del método de investigación.

<!-- La reproducibilidad es clave para verificar y validar nuestros resultados una vez finalizada una investigación, ya que permite reconstruir los datos, los procesos y las decisiones de análisis que condujeron a las conclusiones. Pero también es una forma de trabajo que podemos adoptar desde el inicio: documentar los pasos, fijar versiones de software, automatizar scripts y organizar el código y los datos de manera sistemática. Optar por este camino tiene beneficios importantes: facilita la revisión interna, mejora la colaboración entre coautores, reduce errores difíciles de rastrear y aumenta la credibilidad del trabajo ante la comunidad académica, que puede evaluar, replicar o extender nuestros resultados con mayor confianza. -->

# Modelo de aprendizaje estadístico

En esta sección resumimos el modelo de aprendizaje estadístico que utilizamos como caso de estudio para ilustrar el flujo de trabajo reproducible implementado con Docker. El objetivo no es ahondar en la discusión sustantiva sobre desigualdad económica (desarrollada en detalle en @Machado2023DIE), sino describir los componentes del modelo que son relevantes para su implementación, validación y posterior encapsulamiento en entornos reproducibles.

Partimos de un análisis sobre la desigualdad económica cuyo objetivo es clasificar a los individuos en grupos sociales a partir de variables relevantes para estudiar su participación en el ingreso laboral (@Machado2023DIE). En una primera etapa se aplica un **análisis de clúster**, que permite detectar grupos relativamente homogéneos entre los perceptores de ingresos laborales en un año base, utilizando variables que resumen las características de los trabajadores y de sus puestos de trabajo. En una segunda etapa se emplea **análisis discriminante** para construir, a partir de esos clústeres, una regla de clasificación que pueda aplicarse a otros años y así seguir la evolución de los grupos y su participación en el ingreso laboral. Dado que esta regla se estima en un conjunto de datos y luego se aplica sobre información externa, se utilizan técnicas de **validación cruzada** para evaluar su desempeño y reducir el riesgo de sobreajuste.

A continuación se detallan los datos utilizados y los métodos que forman parte del proceso de implementación en Docker, estos incluyen en particular los componentes de análisis discriminante y validación cruzada. El paso previo de análisis de clúster puede consultarse en el @sec-anexo-cluster.

## Datos

La fuente de datos utilizada es la Encuesta Continua de Hogares (ECH) elaborada por el Instituto Nacional de Estadística (INE). Utilizamos información de personas y hogares para los años 2006, 2011, 2019 y 2021. En cada año se consideran las personas ocupadas de 20 años o más de todo el país con ingresos laborales, definiéndose ocupación como haber trabajado al menos una hora en la semana de referencia. Para el análisis se seleccionan variables que recogen información sobre las tareas realizadas en el trabajo, el sector de actividad, la categoría de ocupación, la informalidad, las horas trabajadas, el nivel educativo, la propiedad de la vivienda y el cobro de rentas. Estos datos constituyen la base sobre la que se estiman el modelo discriminante y los procedimientos de validación descritos a continuación.

El trabajo con datos se realiza íntegramente en R mediante un flujo de trabajo basado en el script `main.R`. La lectura de los archivos originales de la ECH se lleva a cabo con la librería `vroom` (vroom v1.6.6; @vroom), aprovechando su eficiencia para importar grandes volúmenes de datos, mientras que `here` (here v1.0.2; @here) se utiliza para manejar las rutas de archivos de forma relativa al proyecto. La manipulación y depuración de los datos se estructura mediante el operador tubería de `magrittr` (magrittr v2.0.4; @magrittr) y las funciones de `tidytable` (tidytable v0.11.2; @tidytable), que permiten filtrar la población de interés, generar las variables y seleccionar los subconjuntos necesarios para el modelo. Estas dependencias de software forman parte del entorno que luego se fijará explícitamente en el contenedor de Docker.

## Análisis discriminante

El objetivo de esta aplicación es obtener una misma clasificación de grupos para todos los años seleccionados, manteniendo las características distintivas de cada uno. A partir de un conjunto de datos en el que la pertenencia a los grupos es conocida (el año base en el que se aplicó el análisis de clúster), se deriva una regla para asignar las observaciones a los grupos (@mclachlan2005, @hastie2009), minimizando la probabilidad de clasificación incorrecta. En la práctica, se clasifican los datos de 2006, 2019 y 2021 en la misma cantidad de grupos y con la misma caracterización resultante del análisis de clúster realizado en 2011, año elegido como base por presentar la clasificación más estable (véase @sec-anexo-cluster **ARREGLAR PARA QUE LO REFERENCIA COMO ANEXO Y NO COMO SECCIÓN**).

Entre las distintas funciones discriminantes posibles se eligió un modelo de regresión logística multinomial, dado que todas las variables explicativas son cualitativas (@mcfadden1972, @pohar2004, @hosmer2013). Dado que se espera encontrar más de dos grupos, se asume que la variable que indica la subpoblación de pertenencia proviene de una distribución multinomial. Las probabilidades logarítmicas relativas entre cada grupo $k$ y un grupo de referencia $K$ se modelan como (@ISLR):

\begin{center}
    $\log( \frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = \beta_{k0} + \beta_{k1} x_{1} + ... + \beta_{kp} x_{p}$ 
\end{center}

En nuestra especificación se predefinen cuatro grupos ($K = 4$), por lo que dejando fuera al de referencia nos quedan tres funciones de enlace para $k = 1, 2, 3$ ($k = 1, ..., K-1$). Se denotan como $X = (X_{1}, ..., X_{p})$ el conjunto de variables utilizadas, siendo $p$ la cantidad de categorías de todas las variables^[Los parámetros del modelo se estiman bajo el método de máxima verosimilitud con técnicas numéricas de optimización, en concreto el método Broyden-Fletcher-Goldfarb-Shanno.].

Con la información obtenida del año base se pretende discriminar a los individuos de los demás años. Los individuos se asignan a un grupo u otro comparando las probabilidades de pertenencia a cada grupo predichas por el modelo. Según esta regla de clasificación, el grupo asignado es aquel con probabilidad posterior más alta. Para evaluar el modelo se utiliza una medida de precisión definida como la cantidad de datos bien clasificados sobre el total. Valores altos de la misma implican un buen desempeño del modelo.

En términos computacionales, el análisis discriminante se implementa en R utilizando la función `multinom()` de la librería `nnet` (nnet v7.3-20; @MASS, @nnet), que permite estimar el modelo de regresión logística multinomial sobre el conjunto de datos del año base y aplicar posteriormente la regla de clasificación mediante el operador `predict()` de R (R v4.4.1; @R-base) a los demás años. Este modelo y su regla de clasificación constituyen el núcleo del flujo analítico que, en las secciones siguientes, será automatizado y encapsulado en el entorno reproducible definido con Docker.

## Validación cruzada

Como es de interés clasificar datos externos, para evitar problemas de sobreajuste se utiliza el método de \textbf{validación cruzada $k-fold$} (@stone1974, @ISLR). Se dividen los datos aleatoriamente en $k$ pliegues de un tamaño similar, se estima la precisión tomando uno de los pliegues para el testeo y los restantes $k-1$ para el entrenamiento, el procedimiento se repite $k$ veces haciendo variar el pliegue de testeo de tal forma que todos los datos hayan sido utilizados como testeo y entrenamiento. Como resultado se obtienen $k$ estimaciones de la precisión y se toma su promedio como una medida global del ajuste sobre el total de los datos. 

La validación cruzada $k-fold$ se programó íntegramente en R a partir de funciones propias. El script principal `main.R` organiza el procedimiento, llamando a la función de validación `cross_validation()` definida en el script auxiliar `CV.R`. Posteriormente se llama a la función de cálculo de precisión `prf`, definida en el script auxiliar `evaluation.R`. Ambos scripts auxiliares se encuentran en el directorio de dependencias de R del proyecto. Se explota la independencia de los procesos de validación cruzada para correrlos en paralelo mediante las funciones del paquete base `parallel` de R (R v4.4.1; @R-base).

Como se verá en la sección 4, estas decisiones de implementación (paquetes utilizados, estructura de las funciones de validación y uso de computación en paralelo) son las que se fijan posteriormente en el entorno reproducible definido con Docker.

<!-- Dado el volumen de datos y la intensidad computacional requerida, se optó por el uso de Docker para garantizar la reproducibilidad y portabilidad del entorno de desarrollo. Docker nos permitió encapsular todas las dependencias del proyecto dentro de un contenedor, asegurando que el proceso pueda ejecutarse de manera consistente en diferentes entornos, ya sea en servidores en la nube, en un servidor de cómputo local o en computadoras personales con diferentes sistemas operativos (@KaneMatthias2015).-->

<!-- -->

<!-- Para optimizar el rendimiento y facilitar la integración del modelo en entornos de producción, se aplicaron principios de Machine Learning Operations (MLOps). \textbf{MLOps} es un enfoque que combina prácticas de desarrollo de software (DevOps) con metodologías específicas de Machine Learning, con el objetivo de automatizar, escalar y gestionar los ciclos de vida de los modelos de aprendizaje automático de manera eficiente.-->

<!-- Dentro de este marco, el procesamiento en paralelo jugó un papel crucial. Se utilizó el paquete \textbf{parallel} (@RCoreTeam) de R para distribuir el procesamiento de datos y la validación cruzada del modelo en múltiples núcleos de CPU, lo que permitió acelerar significativamente los tiempos de ejecución y manejar grandes volúmenes de datos de manera más efectiva. La integración de estas prácticas asegura que el modelo no solo sea reproducible, sino también escalable y adaptable a diferentes entornos.-->

<!--   -->

<!-- En un proyecto típico combinamos varios lenguajes y herramientas (R para procesar datos y gráficos, Stata para partes del análisis, Python para algún script auxiliar, LaTeX/Quarto para el documento, etc.). Además, solemos tener distintas máquinas involucradas: portátil personal, PC del trabajo, servidor de la facultad o la nube. Docker encaja bien con esa forma de trabajar porque permite definir un entorno único para el proyecto y ejecutarlo igual en todos esos lugares. En vez de escribir instrucciones largas del tipo “instalar tal versión de R, tal paquete, tal versión de Stata”, definimos el entorno una vez, lo empaquetamos y cualquiera puede levantarlo y correr el proyecto completo. -->

<!--  -->

<!-- Existen otros enfoques para gestionar la reproducibilidad en R y otros lenguajes (por ejemplo, sistemas de gestión de dependencias a nivel de paquetes), pero Docker ofrece la ventaja de encapsular el sistema operativo y el conjunto completo de herramientas utilizadas en el proyecto. -->

<!-- Desde el punto de vista operativo, Docker utiliza un servicio del sistema que crea y administra contenedores, apoyándose en el kernel para aislar procesos y recursos, y en un sistema de archivos por capas (copy‑on‑write) que ahorra espacio y acelera las compilaciones. El ciclo de trabajo es simple y repetible: se construye la imagen, se ejecuta montando datos y carpeta de resultados, se observan los registros o se ingresa al contenedor cuando es necesario, se persisten los artefactos en el volumen del equipo anfitrión y, finalmente, se limpian recursos no utilizados. La idea central es que cualquier cambio que ocurra dentro del contenedor es efímero; por ello, los resultados deben escribirse siempre en una ruta montada desde el host (por ejemplo, `/output`).-->

<!-- La diferencia respecto a otros sistemas de virtualización como máquinas virtuales es que Docker comparte el kernel del sistema anfitrión, lo que reduce el uso de recursos y acelera el arranque. Esta característica lo vuelve especialmente adecuado para desarrollar, desplegar y escalar análisis reproducibles en investigación. Las máquinas virtuales, en cambio, cargan un sistema operativo completo, consumen más CPU, memoria y almacenamiento, y requieren herramientas de administración específicas. Para la mayoría de los flujos de investigación orientados a reproducibilidad y portabilidad, los contenedores ofrecen una solución más ágil y suficiente.-->

# Implementación

En esta sección describimos, primero, los componentes principales de Docker que utilizamos, y luego el ejemplo concreto de Dockerfile y comandos para replicar el análisis.

**LA IDEA ES HACER UN PREAMBULO, QUEDÓ MEDIO CORTO**

## Componentes de Docker

En el contexto de la investigación empírica, Docker permite “congelar” un entorno de trabajo completo (sistema operativo, librerías, versiones de R, Stata, Python, LaTeX/Quarto, etc.) y reutilizarlo o compartirlo sin depender de la configuración específica de cada máquina. En la práctica, lo que hace Docker es ejecutar estos programas dentro de un entorno Linux aislado, pero nosotros seguimos trabajando desde nuestro navegador o editor habitual (como RStudio, Positron o VS Code), conectándonos al contenedor. 

Docker funciona como una capa de abstracción a nivel del sistema operativo, que permite empaquetar aplicaciones y sus dependencias en contenedores ligeros y portables. Se apoya en el kernel para aislar procesos y recursos, y en un sistema de archivos por capas (copy‑on‑write) que ahorra espacio y acelera las compilaciones. De este modo, el software se ejecuta de forma consistente en distintos entornos (equipo local, servidores o nube). Docker se ejecuta mediante comandos en la terminal (CLI) o a través de interfaces gráficas (Docker Desktop) y consta de varios componentes clave que facilitan la creación, distribución y ejecución de contenedores.

Los principales componentes de Docker son:

- `Dockerfile`: archivo de texto con instrucciones para construir la imagen.
- Imagen: plantilla estática y versionada del entorno completo.
- Contenedor: instancia en ejecución de la imagen.
- Registro: repositorio para almacenar y compartir imágenes (Docker Hub, GHCR).

El flujo presentado en la Figura \ref{fig:dockerlifecycle} permite integrar los componentes y ver como se interrelacionan en el ciclo de vida de Docker. Una imagen es la plantilla inmutable del entorno (sistema, librerías y, cuando corresponde, parte del código) identificada por `repo:tag` y por un `digest` (`sha256:`) que garantiza identidad exacta entre máquinas. Un contenedor es el proceso que ejecuta esa imagen de forma aislada; toda escritura dentro del contenedor es temporal, salvo que se utilice un volumen montado desde el equipo anfitrión, que es donde deben guardarse datos y resultados. El Dockerfile es la "receta" para construir la imagen con instrucciones como `FROM`, `RUN` o `COPY`, y el registro es el repositorio donde se versionan y comparten imágenes con coautores y replicadores. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/docker-lifecycle.png}
  \caption{Ciclo de vida simplificado de Docker.}
  \label{fig:dockerlifecycle}
\end{figure}

**CAPAZ AGREGAR ACÁ UNA DESCRIPCIÓN MUY PARA TONTOS DE TODO EL FLUJO QUE SE PRESENTA EN LA IMAGEN, SOBRE TODO PORQUE HAY COSAS QUE SE NOMBRAN EN LA IMAGEN Y NO EN EL TEXTO, LA EXPLICACIÓN DEBERÍA ABARCAR TODOS LOS CONCETOS QUE ESTÁN EN EL DIAGRAMA**

Para sostener la reproducibilidad a lo largo del tiempo, se recomienda etiquetar imágenes con fecha o versión (y, cuando sea crítico, referenciarlas por `digest`), congelar paquetes (R: `renv`; Python: `requirements.txt`) y guardar `sessionInfo()` o `pip freeze` junto a los resultados. También es recomendable limitar recursos (CPU y memoria) para obtener ejecuciones controladas y comparables. Los datos y las credenciales no deben incorporarse en la imagen: se montan a través de un volumen con `-v` y se suministran por variables de entorno o archivos `.env`. También es conveniente fijar semillas aleatorias, normalizar configuraciones regionales y zonas horarias y ejecutar como usuario no privilegiado (o con `--user $(id -u):$(id -g)`). Documentar los comandos exactos de `docker build` y `docker run` (y los límites de recursos) en un README o Makefile mejora la trazabilidad.


El [bloque de código @lst-commands-utils] contiene los comandos esenciales para el uso de Docker para inspeccionar, ejecutar, observar y limpiar:

::: {#lst-commands-utils}

```bash
# Inspección
docker images # Lista todas las imágenes locales
docker ps -a # Lista todos los contenedores (también los detenidos)

# Ejecutar con límites y variables
docker run --rm -it -v "$PWD":/work -w /work --cpus=4 -m 8g \
  --env-file .env ghcr.io/fabriciomch/cv:latest /bin/bash

# Logs y acceso
docker logs -f <nombre_o_id> # Seguir logs en tiempo real
docker exec -it <nombre_o_id> /bin/bash # Acceder a la shell del contenedor

# Copiar artefactos (scripts, resultados, etc.)
docker cp <nombre_o_id>:/work/output/resultados.csv ./output/

# Limpieza
docker system prune -f # Limpia volúmenes, redes y contenedores detenidos
docker image prune -f # Limpia imágenes no usadas
```

:::

A modo de cierre, es útil recordar algunas pautas para evitar errores frecuentes: utilizar siempre una imagen etiquetada y el Dockerfile versionado; fijar dependencias y semillas; montar datos y escribir resultados en `/work/output`; documentar el comando de ejecución y los recursos asignados; y conservar registros y metadatos de sesión junto a los outputs. Si los resultados no aparecen, suele deberse a que no se montó `-v "$PWD":/work` o no se escribió en la ruta montada; si faltan paquetes, deben instalarse en la imagen base o en un paso de inicialización; si surgen problemas de permisos, ejecutar con `--user $(id -u):$(id -g)`; y si hay limitaciones de memoria, aumentar `-m` o reducir el tamaño de los lotes.

**FIJATE SI ALGUNO DE LOS BLOQUES DE TEXTO QUE ESTÁN COMENTADOS ANTES DE QUE ARRANQUE LA SECCIÓN DEBERÍA IR, SI SON IMPORTANTES HABRÍA QUE INTEGRARLOS PERO CUIDANDO EL LARGO DE LAS SUBSECCIONES Y EL HILO**

## Ejemplo 

El ambiente de trabajo se configuró mediante un `Dockerfile` que define todas las herramientas y bibliotecas necesarias para la ejecución del modelo. Este archivo incluye desde la instalación de R y sus paquetes relevantes, hasta la configuración de las dependencias del sistema operativo. De esta forma, cualquier usuario puede reproducir el ambiente exacto simplemente construyendo y ejecutando el contenedor.

Como ejemplo concreto, a continuación se muestra un Dockerfile (ver [bloque de código @lst-dockerfile-example]) para obtener el resultado del análisis detallado en la sección **PONER REF**. Usa como base `eddelbuettel/r2u:22.04` (Ubuntu + R con binarios precompilados), instala los paquetes mencionados en **poner ref** (`nnet`, `vroom`, `here`, `tidytable`, `magrittr`), crea una estructura de carpetas para el proyecto e incluye los datos (`/home/ubuntu/model/{data,R,output}`) a utilizar, instala paquetes de R, copia archivos y define un comando por defecto que ejecuta el script principal.


::: {#lst-dockerfile-example}

```dockerfile
FROM eddelbuettel/r2u:22.04  # Ubuntu 22.04

# Estructura de proyecto
RUN mkdir -p /home/ubuntu/model/{data,R,output}
WORKDIR /home/ubuntu/model

# Paquetes de R del sistema (binarios)
RUN apt-get update && apt-get install -y \
    r-cran-nnet \
    r-cran-vroom \
    r-cran-here \
    r-cran-tidytable \
    r-cran-magrittr \
  && rm -rf /var/lib/apt/lists/*

# Copiar archivos (opcional; preferible montar con -v en ejecución)
COPY R/* R/
COPY data/ data/ 
COPY main.R main.R

# Variables de entorno de ejemplo
ARG RUN_MODE=prod
ENV RUN_MODE=${RUN_MODE}

# Comando por defecto (En este script se ejecutan los modelos con CV)
CMD ["Rscript", "main.R"]
```
:::


Una vez creada la imagen a partir de este `Dockerfile`, los contenedores ejecutarán el script `main.R`, que implementa el modelo de aprendizaje estadístico con validación cruzada y guarda los resultados en la carpeta `output/`. El comando por defecto puede ser sobrescrito al ejecutar el contenedor (mediante `docker run`) si se desea correr otro script o ingresar a una shell interactiva (`docker run imagen -it bash`) dentro del contenedor.

Al construir esta imagen, los comandos de ejecución deben montar el proyecto en la misma ruta definida como `WORKDIR` para que los scripts y outputs queden en el lugar esperado. Como puede verse en el [bloque de código @lst-dockerfile-usage]. 

::: {#lst-dockerfile-usage}
```bash
# Construir la imagen local
docker build -t cv_multinom .

# Abrir una shell en la ruta del proyecto dentro del contenedor
docker run --rm -it \
  -v "$PWD":/home/ubuntu/model -w /home/ubuntu/model \
  cv_multinom /bin/bash

# Ejecutar el script principal y escribir resultados en ./output
docker run --rm \
  -v "$PWD":/home/ubuntu/model -w /home/ubuntu/model \
  cv_multinom Rscript main.R
```
:::

Este Dockerfile tiene un ejemplo básico para que devuelva el accuracy del modelo, definido en la sección **poner ref** como la forma correcta de validar los resultados. En otros casos puede ser util guardar otras métricas, modelos intermedios o datos procesados en la carpeta `output/`, que está montada desde el equipo anfitrión.

La imagen ya se encuentra construida y publicada en base a la ultima versión del código en el repositorio de [GitHub](https://github.com/FabricioMch/CV) y mediante GitHub Actions se actualiza automáticamente al crear nuevas etiquetas.


# Ejecución

Para trabajar con la imagen pública de este proyecto alojada en GitHub Container Registry (GHCR: `ghcr.io/fabriciomch/cv`) no es necesario modificar nada: basta con descargarla, ejecutarla montando la carpeta del proyecto y, si se desea mantener una variante local, construir una imagen propia a partir de un Dockerfile. De manera esquemática, un flujo típico de trabajo puede resumirse del siguiente modo. 

Pasos de uso (imagen GHCR):

1. Obtener la imagen
   - Para imágenes públicas (como en este caso), ejecutar:
     ```bash
     docker pull ghcr.io/fabriciomch/cv:latest
     ```
   - Si fuera privada, autenticarse previamente en GitHub Container Registry (GHCR) con un personal access token (PAT) asociado a la cuenta de GitHub.

2. Vincular el proyecto local
   - Desde la carpeta del proyecto (donde residen código y datos), ejecutar:
     ```bash
     docker run --rm -it -v "$PWD":/work -w /work ghcr.io/fabriciomch/cv:latest /bin/bash
     ```

3. Ejecutar el script de análisis
   - Por ejemplo:
     ```bash
     docker run --rm -v "$PWD":/work -w /work ghcr.io/fabriciomch/cv:latest Rscript main.R
     ```

4. (Opcional) Congelar una variante propia
   - Clonar el repositorio, adaptar el Dockerfile si es necesario y construir:
     ```bash
     docker build -t cv_multinom --build-arg test_arg=TRUE .
     ```


Para la ejecución cotidiana y colaborativa, resulta útil acompañar el flujo de trabajo con herramientas auxiliares. La integración continua (GitHub/GitLab CI) permite construir y publicar imágenes automáticamente al crear etiquetas; un Makefile facilita encapsular comandos repetitivos como `docker run` o `docker build`; y servicios como Amazon EC2 o AWS Batch resultan apropiados para ejecutar los mismos contenedores cuando se requiere mayor capacidad de cómputo. 

Para obtener resultados de forma consistente:

- Descargar o actualizar la imagen y verificar la etiqueta/versión utilizada.
- Ejecutar el contenedor montando el proyecto y escribir los outputs en la carpeta compartida.
- Registrar logs, versiones de paquetes y metadatos de sesión junto a los resultados.


Para ilustrar la simplicidad de uso, el [bloque de código @lst-dockerfile-minimal] muestra los comandos mínimos para descargar y ejecutar la imagen pública del proyecto, montando la carpeta `output` para guardar los resultados generados por el script principal.

::: {#lst-dockerfile-minimal}
```bash
docker pull ghcr.io/fabriciomch/cv:main
docker run -v ${PWD}/output/:/home/ubuntu/model/output ghcr.io/fabriciomch/cv:main
```
:::

Este comando descarga la imagen pública y ejecuta el contenedor, montando la carpeta `output` del proyecto local para guardar los resultados generados por el script principal. Al tener en cuenta las buenas prácticas mencionadas, se asegura que los análisis sean reproducibles y portables entre distintos entornos y colaboradores.

## Desarrollo en la nube con GitHub Codespaces

Una vez que se cuenta con la imagen Docker, existen múltiples formas de utilizarla para ejecutar el análisis de forma reproducible tanto en entornos locales (utilizando Docker Desktop o Docker Engine) como en la nube como GitHub Codespaces.

GitHub Codespaces es un servicio que permite abrir un entorno de desarrollo completo en la nube, basado en Visual Studio Code, directamente desde un repositorio de GitHub. Este entorno puede configurarse para utilizar contenedores Docker como base, lo que facilita la ejecución de análisis reproducibles sin necesidad de instalar software adicional en el equipo local. 

A continuación se describen los pasos básicos para utilizar la imagen Docker del proyecto en un entorno de GitHub Codespaces (Figura \ref{fig:opencodespace}). Una vez abierto el Codespace, se puede trabajar directamente con el código. El repositorio incluye un editor de texto listo para ejecutar el contenedor Docker (Figura \ref{fig:githubcodespace}). Esto puede ser especialmente útil para colaborar con otros investigadores, docentes, alumnos, ya que todos pueden acceder al mismo entorno sin preocuparse por las diferencias en la configuración local de sus equipos. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/openCodeSpace.png}
  \caption{Apertura de un GitHub Codespace desde la interfaz del repositorio.}
  \label{fig:opencodespace}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/GithubCodeSpace.png}
  \caption{Entorno de desarrollo en la nube provisto por GitHub Codespaces.}
  \label{fig:githubcodespace}
\end{figure}

GitHub Codespaces puede entenderse, en este contexto, como un complemento a los flujos basados en contenedores: permite abrir un entorno de desarrollo efímero en la nube, definido a partir del propio repositorio (por ejemplo mediante un archivo de configuración `.devcontainer.json`), en el que se puede inspeccionar el código, ajustar scripts y lanzar contenedores Docker con el mismo entorno que se utiliza localmente.

Esto resulta especialmente útil para trabajo colaborativo, docencia o talleres, ya que cualquier integrante del equipo puede acceder a un entorno listo para usar sin instalar herramientas en su equipo personal. No obstante, es importante tener en cuenta que GitHub Codespaces presenta ciertas limitaciones en cuanto a recursos (CPU, memoria y almacenamiento) y tiempo de ejecución, por lo que puede no resultar adecuado para análisis muy intensivos o de larga duración. Servicios de cómputo en la nube como Amazon EC2 u otras plataformas de infraestructura como servicio suelen ser más apropiados cuando se necesita alquilar capacidad de procesamiento para ejecutar cargas pesadas en segundo plano (por ejemplo, corridas extensivas de validación cruzada, simulaciones o entrenamientos de modelos de mayor porte). En estos casos, la práctica recomendable es mantener la lógica del análisis y la definición del entorno en una imagen Docker versionada, y desplegar esa misma imagen tanto en el entorno local como en instancias de cómputo en la nube, de modo que la decisión sobre el ‘dónde’ ejecutar (equipo propio o nube) no afecte al ‘cómo’ se ejecuta el análisis.







# Comentarios finales

Este trabajo presentó un estudio de caso en el que se documenta el uso de entornos reproducibles basados en Docker para la implementación y validación de un modelo de aprendizaje estadístico en ciencias sociales. Partiendo de una investigación sobre la distribución del ingreso laboral y la clasificación de trabajadores en grupos socioeconómicos, mostramos cómo traducir un flujo de análisis concreto, que combina datos de encuestas de hogares, regresión logística multinomial y validación cruzada, a un entorno de cómputo fijado, portable y ejecutable en distintos contextos. En la aplicación empírica, el modelo discriminante alcanzó una **precisión del 94,35%**, desempeño al que puede llegar cualquier usuario que replique el proceso siguiendo los pasos que detallamos.

El aporte es doble. En primer lugar, proponemos un flujo de trabajo reproducible que integra de manera explícita el modelo de aprendizaje estadístico, los paquetes de R utilizados y el entorno de software subyacente, encapsulados en una imagen Docker versionada. En segundo lugar, acompañamos este diseño con una guía práctica que va desde la definición del Dockerfile hasta la ejecución del análisis mediante una imagen pública alojada en GitHub Container Registry, incluyendo ejemplos de uso local y en la nube (por ejemplo GitHub Codespaces, Gitpod o instancias en Amazon EC2). De este modo, el trabajo no se limita a argumentar a favor de la reproducibilidad, sino que muestra paso a paso cómo alcanzarla en una aplicación empírica concreta.

En términos prácticos, la experiencia de implementación dejó beneficios claros. La encapsulación del entorno redujo fricciones habituales al mover el proyecto entre equipos y sistemas operativos, facilitó la reejecución del análisis en distintos momentos del tiempo y en distintas máquinas, y simplificó la colaboración con replicadores. El uso combinado de contenedores, scripts organizados y validación cruzada programada permitió automatizar tareas costosas, controlar de forma más precisa las dependencias de software y asegurar que el “cómo” se ejecuta el análisis permanezca estable aunque cambie el “dónde” se ejecuta (máquina personal, servidor institucional o nube). 

Hemos comprobado que Docker puede ser una herramienta valiosa en la investigación en estadística y ciencias sociales. Sus principales fortalezas son la reproducibilidad, el aislamiento de dependencias, la portabilidad y la eficiencia computacional, especialmente cuando se trabaja con combinaciones de datos y métodos de alta exigencia computacional o cuando se detectan espacios para paralelizar los procesos. La experiencia documentada muestra que la adopción de contenedores Docker aporta beneficios tangibles para proyectos de métodos cuantitativos: reduce fricciones en la instalación de dependencias, estabiliza versiones de software, facilita la colaboración entre equipos y mejora la trazabilidad de los resultados. Al encapsular el entorno de ejecución, los análisis pueden replicarse en el tiempo y compartirse con terceros sin la carga de reconstruir manualmente las condiciones de trabajo.

Al mismo tiempo, el enfoque presentado no pretende cubrir todas las situaciones posibles. El caso de estudio se centra en un modelo relativamente acotado, implementado en un único lenguaje (R) y sobre un conjunto de datos específico, por lo que ciertos desafíos que enfrentan proyectos más grandes, multicéntricos o multilanguage quedan fuera del alcance de este ejercicio. La adopción de Docker y herramientas afines también supone familiarizarse con algunos conceptos básicos de contenedores y con la línea de comandos, lo que puede requerir un período de adaptación. A ello se suman consideraciones habituales en proyectos con datos reales, como las restricciones de confidencialidad o licencias, y condicionantes de infraestructura en ciertos entornos de cómputo, en particular cuando se trabaja con recursos gratuitos o limitados en la nube. Más que obstáculos insalvables, estos elementos señalan ámbitos en los que es necesario acompañar la adopción de contenedores con materiales de apoyo, capacitación y buenas prácticas de gestión de datos.

Estas consideraciones abren, a su vez, líneas posibles de extensión. Una primera dirección consiste en generalizar el enfoque a otros tipos de modelos y lenguajes (por ejemplo, combinar R, Python y Stata en un mismo contenedor, o integrar etapas adicionales de preprocesamiento, modelado y visualización). Una segunda línea apunta a profundizar la automatización mediante herramientas de gestión de dependencias (como renv en R), pruebas automatizadas y pipelines de integración continua que construyan y verifiquen imágenes con cada cambio relevante en el código. Finalmente, a nivel de comunidad, el uso de plantillas de proyectos, repositorios de ejemplo y actividades de formación podría facilitar una adopción progresiva de estos entornos reproducibles en grupos de investigación, cursos de grado y posgrado o instancias de docencia aplicada.

En conjunto, el caso presentado sugiere que los contenedores de software pueden desempeñar un papel central en la traducción de los principios de ciencia abierta y reproducibilidad computacional a prácticas concretas de trabajo con datos. Al fijar el entorno, automatizar los pasos clave del análisis y documentar de forma clara cómo ejecutar el flujo completo, se refuerza la posibilidad de verificar, compartir y extender los resultados de la investigación empírica. En síntesis, el uso de ambientes reproducibles con Docker constituye una práctica recomendable para fortalecer la solidez empírica, la transparencia y la portabilidad de los análisis en contextos académicos, a la vez que habilita escalabilidad y mantenimiento a lo largo del ciclo de vida de los proyectos.

\newpage

## Anexo A. Análisis de cluster {#sec-anexo-cluster}

Mediante esta técnica se pretende clasificar individuos en grupos (@blanco2006, @kaufman2009). Para esto es necesario definir una métrica que cuantifique la similitud entre los individuos de la población. La variedad de métricas tienen distinta sensibilidad a los datos atípicos, así como difieren en el tipo de variables que pueden analizar entre cuantitativas y categóricas, nominales u ordinales.

Dado que se trabajará con variables categóricas la métrica elegida es el coeficiente de similitud de Jaccard, definido como el tamaño de la intersección dividido por el tamaño de la unión de los conjuntos de muestras (@Jaccard1901, @Jaccard1912):

\begin{equation*}
  J(A,B) = \frac{|A \bigcap  B|}{|A \bigcup B|} = \frac{|A \bigcap  B|}{|A| + |B| - |A \bigcap B|}
\end{equation*}

En nuestra aplicación, las observaciones se representan como vectores de indicadores de categorías (presencia/ausencia), y el coeficiente de Jaccard mide cuánta coincidencia hay en esas categorías entre dos individuos. Así, individuos que comparten un conjunto similar de categorías (por ejemplo, sector, tamaño de empresa, tipo de vínculo) obtendrán valores altos de similitud y tenderán a ser asignados al mismo grupo. De esta forma luego del análisis individuos que compartan un conjunto de categorías estarán asignados a un mismo grupo (@Realvargas). Analizando posteriormente las características de los individuos pertenecientes a cada grupo, se podrá llegar a una caracterización a partir de la cual compararlos. 

Se opta por métodos de clusterización no jerárquicos debido al tamaño de las bases de datos a utilizar (@guenoche1991). Los métodos no jerárquicos se caracterizan por comenzar con una primera solución de clasificación, y luego de forma iterativa aplicar diferentes combinaciones para obtener particiones más homogéneas.

El método a utilizar es K-Medoides o partición en torno a los medoides (PAM) tal como presentan @kaufman2009, donde cada clúster está representado por un medoide, es decir, un individuo representativo del grupo. Un medoide es un objeto representativo del conjunto de datos, tal que su diferencia promedio con los demás objetos es mínima. Con la cantidad $k$ de grupos definida previamente se asignan $k$ medoides dentro de los $n$ individuos, los demás individuos son asignados al grupo del medoide más cercano. La asignación de medoides se actualiza iterativamente, minimizando la suma de distancias entre los individuos de cada grupo y su medoide. Como el arranque es aleatorio se recomienda aplicar más de una vez el método para llegar a conclusiones válidas. 

Dado el tamaño de los datos a utilizar se optará por la implementación CLARA (@kaufman2009) de K-Medoides, que  utiliza el enfoque de muestreo repetido. Este enfoque ha sido ampliamente utilizado y extendido en aplicaciones con grandes volúmenes de datos (@Park2009). Considera una muestra fija de los datos y aplica en ella el algoritmo PAM encontrando el conjunto óptimo de medoides para la muestra. Se repite el proceso de muestreo y agrupación un número predefinido de veces para minimizar el sesgo del muestreo. La cantidad de muestras se definirá probando un valor creciente de las mismas y verificando a partir de cual los resultados se estabilizan. 

Como se mencionó anteriormente, este tipo de métodos depende de predefinir la cantidad de grupos. Una alternativa para esto es maximizar la silueta promedio. La silueta es un estadístico que determina cuán similar es un individuo respecto al grupo al que pertenece en comparación a los demás grupos. Presenta un rango acotado en valor absoluto menor a la unidad. Un valor alto indica que el individuo es muy similar al grupo, y un valor bajo indica que el individuo es muy diferente al grupo. En términos funcionales puede expresarse como:

\begin{equation*}
  s(i) = \frac{b(i) - a(i)}{max\{a(i), b(i)\}}
\end{equation*}

donde $a(i)$ es la distancia promedio de la observación $i$ a los objetos de su grupo y $b(i)$ es la distancia promedio de la observación $i$ a los objetos de los demás grupos (distancia intra y entre grupos). A nivel agregado se utiliza la silueta promedio para una cantidad dada de grupos, calculada como la media de las siluetas individuales. Para obtener una recomendación sobre la cantidad de grupos óptima en este tipo de algoritmos se puede comparar la silueta promedio de cada configuración. Aquella que obtenga un valor más alto será una buena candidata a cantidad de grupos. 

Dadas las particularidades del año 2021 (@INE2020ECH) se decidió no tomarlo como año base para aplicar el análisis de cluster. Los datos de 2006 fueron descartados para este análisis por la constatación de datos faltantes en la variable referente al tamaño de la empresa, considerada importante para diferenciar los grupos. Los análisis de clúster realizados para 2011 y 2019 muestran una mejor separación de grupos en el año 2011, con una silueta promedio más elevada y sin problemas relevantes de solapamiento entre grupos específicos. Se eligen por lo tanto los datos del año 2011 para realizar el análisis de cluster y servir de año base para los análisis posteriores. 

\newpage

# Referencias



